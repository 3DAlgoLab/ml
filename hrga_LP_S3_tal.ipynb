{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # Milestone 3: Training a Supervised Image Classifier on the Melanoma Dataset using Transfer Learning\n","\n"," In machine learning, there are no ready-to-use recipes. Certain approaches have been found to work well for certain cases, but in the end, much of it comes down to trial and error. [Occam’s razor](https://en.wikipedia.org/wiki/Occam%27s_razor) (the principle roughly summarized as “the simplest solution is usually the best one”) suggests starting from the most basic method, and building up on top of it until we get the performance we want. Keeping track of improvements we get at each step also helps us justify our efforts!\n","\n"," ## Part A: The Data\n","\n"," Here I reuse the Dataset code that I wrote for Milestone 2.\n","\n"," Even in the extreme situation of only having 200 labeled samples available for training, we still have to set aside some number of them for the validation set. The fewer training samples we have, the greater the risk of overfitting, which makes having a validation set absolutely essential. It is up to you to decide what fraction of the precious few labeled samples to keep for validation purposes; I set it at 30% or 60 images, as anything smaller than that would likely result in a major val set overfitting problem."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Current Device:  cuda\n"]}],"source":["import os\n","from os import listdir\n","from os.path import join\n","\n","from PIL import Image\n","import random\n","\n","import matplotlib.pyplot as plt\n","\n","\n","import numpy as np\n","import copy\n","\n","import torch \n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import  models\n","import torch.nn.functional as F\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Current Device: \", device)\n","\n","from pathlib import Path"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Define a function to set the seed. This will make results reproducible.\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","            \n","    # [your code here]\n","\n","# Define a function to check if a file is actually an image.\n","def is_image(filename):\n","    if isinstance(filename, str):\n","        filename = Path(filename)\n","    return filename.suffix in ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Some test for Path \n","\n","dir_labeled = \"./data/MelanomaDetectionLabeled/labeled/\"\n","dir_test  = \"./data/MelanomaDetectionLabeled/test/\""]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of image is 200\n","Label: 0\n"]},{"data":{"text/plain":["array([[[175, 138, 129],\n","        [178, 142, 130],\n","        [183, 147, 135],\n","        ...,\n","        [176, 148, 134],\n","        [170, 142, 128],\n","        [162, 134, 122]],\n","\n","       [[185, 151, 141],\n","        [178, 141, 132],\n","        [172, 136, 124],\n","        ...,\n","        [176, 148, 134],\n","        [170, 142, 130],\n","        [158, 130, 118]],\n","\n","       [[186, 152, 143],\n","        [176, 142, 133],\n","        [172, 138, 128],\n","        ...,\n","        [176, 146, 135],\n","        [172, 144, 132],\n","        [163, 135, 123]],\n","\n","       ...,\n","\n","       [[166, 135, 140],\n","        [164, 134, 136],\n","        [153, 123, 123],\n","        ...,\n","        [160, 131, 117],\n","        [156, 125, 107],\n","        [149, 118,  98]],\n","\n","       [[164, 135, 139],\n","        [165, 136, 138],\n","        [159, 130, 132],\n","        ...,\n","        [162, 128, 118],\n","        [154, 121, 106],\n","        [146, 110,  94]],\n","\n","       [[144, 115, 119],\n","        [155, 129, 132],\n","        [161, 135, 136],\n","        ...,\n","        [161, 124, 115],\n","        [153, 117, 103],\n","        [143, 105,  92]]], dtype=uint8)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["class LabeledDataset(Dataset):\n","\n","    def __init__(self, dir_path, transform=None):\n","        \"\"\"\n","        Args:\n","            dir_path (string): Directory containing the images.\n","            transform (optional): Optional transform to be applied\n","                on an image.\n","        \"\"\"\n","        \n","        # The list of all the image file names (but not the images themselves!) will be read\n","        # when the Dataset object is initialized\n","        p = Path(dir_path).resolve()\n","        self.img_path_list = [f for f in p.iterdir() if is_image(f)]\n","        self.transform = transform\n","        \n","        \n","    def __len__(self):\n","        return len(self.img_path_list)    \n","\n","    def __getitem__(self, idx):\n","        \n","        # Here is where the image actually gets read:\n","        img_path = self.img_path_list[idx]\n","        # img = read_image(str(img_path)).type(torch.float32)/255.0        \n","        img = Image.open(img_path)\n","        if self.transform:\n","            img = self.transform(img)\n","            \n","        img = np.asarray(img)\n","        label = int(img_path.stem.split('_')[1])\n","\n","        return img, label\n","    \n","    \n","# test code \n","\n","labeled_set = LabeledDataset(dir_labeled)\n","\n","print('Number of image is {}'.format(len(labeled_set)))\n","print('Label:', labeled_set[0][1])\n","\n","labeled_set[100][0]"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Could not infer dtype of numpy.uint8","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m labeled_loader \u001b[39m=\u001b[39m DataLoader(labeled_set, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m labeled_loader:\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape)\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:143\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    140\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:120\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 120\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39mas_tensor(b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m--> 172\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map)\n","\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of numpy.uint8"]}],"source":["labeled_loader = DataLoader(labeled_set, batch_size=4, shuffle=True)\n","\n","for batch in labeled_loader:\n","    print(batch[0].shape)\n","    print(batch[1].shape)\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","for batch_idx, batch in enumerate(labeled_loader):    \n","    if batch_idx > 0:\n","        break \n","    \n","    print(\"Batch labels\", batch[1].data)\n","    image_grid = torchvision.utils.make_grid(batch[0], nrow=4)\n","    plt.imshow(image_grid.permute(1, 2, 0))\n","    plt.pause(0.05)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCell was canceled due to an error in a previous cell."]}],"source":["# Choose some transformations\n","rotation = transforms.RandomChoice([transforms.RandomRotation([-3, 3]), \n","                                    transforms.RandomRotation([87, 93]), \n","                                    transforms.RandomRotation([177,183]),\n","                                    transforms.RandomRotation([267, 273])])\n","augmentation = transforms.Compose([transforms.RandomHorizontalFlip(), \n","                                   transforms.RandomVerticalFlip(), \n","                                   rotation])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCell was canceled due to an error in a previous cell."]}],"source":["# Instantiate the Label Dataset class for training\n","labeled_set = LabeledDataset(dir_labeled, transform=transforms.Compose([transforms.ToTensor(), augmentation]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCell was canceled due to an error in a previous cell."]}],"source":["# Randomly split the dataset (don't forget to set the seed!)\n","set_seed(123)\n","\n","train_set, val_set = torch.utils.data.random_split(labeled_set, [0.7, 0.3])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCell was canceled due to an error in a previous cell."]}],"source":["# Instantiate the LabeledDataset class for testing\n","test_set = LabeledDataset(dir_test, transform=transforms.ToTensor())\n","\n","# Print the number of images in the train, validation and test sets\n","print(\"Number of images in the training set: \", len(train_set))\n","print(\"Number of images in the validation set: \", len(val_set))\n","print(\"Number of images in the test set: \", len(test_set))\n","\n","# Write data loaders for training, validation and testing\n","# Since we have so few samples for the fully supervised part, you can use the entire set \n","# at each training iteration, rather than separate it into mini-batches:\n","\n","# [your code here]"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Numpy is not available","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\dev_repo\\ml\\hrga_LP_S3_tal.py\u001b[0m in \u001b[0;36mline 4\n\u001b[0;32m      <a href='file:///c%3A/dev_repo/ml/hrga_LP_S3_tal.py?line=43'>44</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/dev_repo/ml/hrga_LP_S3_tal.py?line=45'>46</a>\u001b[0m labeled_set \u001b[39m=\u001b[39m LabeledDataset(dir_labeled, transform\u001b[39m=\u001b[39mtransforms\u001b[39m.\u001b[39mToTensor())\n\u001b[1;32m----> <a href='file:///c%3A/dev_repo/ml/hrga_LP_S3_tal.py?line=46'>47</a>\u001b[0m img \u001b[39m=\u001b[39m labeled_set[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='file:///c%3A/dev_repo/ml/hrga_LP_S3_tal.py?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(img)\n","\u001b[1;32mc:\\dev_repo\\ml\\hrga_LP_S3_tal.py\u001b[0m in \u001b[0;36mline 30\u001b[0m, in \u001b[0;36mLabeledDataset.__getitem__\u001b[1;34m(self, idx)\n\u001b[0;32m     <a href='file:///c%3A/dev_repo/ml/hrga_LP_S3_tal.py?line=27'>28</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\n\u001b[0;32m     <a href='file:///c%3A/dev_repo/ml/hrga_LP_S3_tal.py?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m---> <a href='file:///c%3A/dev_repo/ml/hrga_LP_S3_tal.py?line=29'>30</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m     <a href='file:///c%3A/dev_repo/ml/hrga_LP_S3_tal.py?line=30'>31</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(img_path\u001b[39m.\u001b[39mstem\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='file:///c%3A/dev_repo/ml/hrga_LP_S3_tal.py?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m img, label\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/transforms.py?line=126'>127</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/transforms.py?line=127'>128</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/transforms.py?line=128'>129</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/transforms.py?line=129'>130</a>\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/transforms.py?line=132'>133</a>\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/transforms.py?line=133'>134</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/transforms.py?line=134'>135</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n","File \u001b[1;32mc:\\Users\\three\\miniconda3\\envs\\cam\\lib\\site-packages\\torchvision\\transforms\\functional.py:163\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/functional.py?line=160'>161</a>\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/functional.py?line=161'>162</a>\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[1;32m--> <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/functional.py?line=162'>163</a>\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/functional.py?line=164'>165</a>\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/three/miniconda3/envs/cam/lib/site-packages/torchvision/transforms/functional.py?line=165'>166</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n","\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"]}],"source":["labeled_set = LabeledDataset(dir_labeled, transform=transforms.ToTensor())\n","img = labeled_set[0][0]\n","print(img)"]}],"metadata":{"kernelspec":{"display_name":"cam","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d573937473b88991df033c8e7cd2a78641f250341eab636e267870441ea13f65"}}},"nbformat":4,"nbformat_minor":2}
